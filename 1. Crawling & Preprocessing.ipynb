{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawling & Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVePtg6zY_he"
      },
      "source": [
        "!pip install sastrawi\n",
        "!pip install swifter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo_Hsi3XTmEh"
      },
      "source": [
        "import pandas as pd\n",
        "import tweepy as tw\n",
        "import math\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import swifter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV3Uh1znfL_w"
      },
      "source": [
        "api_key = 'FSSjIX9PWtgPkmxAZr5oNN70v'\n",
        "api_key_secret = 'ySTNsffGwh8IlALmnhyzQN0gCsD4RF3gW75rQZDNtk2402K5FK'\n",
        "access_token = '1314360748865249280-xTyYCvheTzvcvo6RNYD7XUSorTRvB3'\n",
        "access_token_secret = 'lSvETMGce9Q9hNr5QUEOyeZ5YBY3ohE5eoyQcoMn5VHgu'\n",
        "\n",
        "auth = tw.OAuthHandler(api_key, api_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tw.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        "\n",
        "def get_tweet(query, total):\n",
        "\n",
        "  data_tweets = []\n",
        "  places = api.geo_search(query=\"Indonesia\", granularity=\"country\")\n",
        "  place_id = places[0].id\n",
        "  \n",
        "  filter_query = query + ' place:%s' % place_id +' -filter:retweets'\n",
        "  \n",
        "\n",
        "  while(total > 0):\n",
        "    iteration = math.ceil(total / 100)\n",
        "\n",
        "    for x in range(0, iteration):\n",
        "      if x == 0:\n",
        "        tweets = api.search(\n",
        "            q=filter_query,\n",
        "            lang='id',\n",
        "            count=(100 if total >= 100 else total),\n",
        "            tweet_mode='extended'\n",
        "        )\n",
        "      else:\n",
        "        tweets = api.search(\n",
        "            q=filter_query,\n",
        "            lang='id',\n",
        "            count=(100 if total >= 100 else total),\n",
        "            max_id=str(max_id - 1),\n",
        "            tweet_mode='extended'\n",
        "        )\n",
        "      \n",
        "      data_tweets += [[tweet.id, tweet.user.name, tweet.user.location, tweet.full_text, tweet.created_at] for tweet in tweets]\n",
        "      max_id = tweets[-1].id\n",
        "\n",
        "      total = total - len(tweets)\n",
        "\n",
        "  return data_tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyWsUlhDUUve"
      },
      "source": [
        "def remove_punct(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?:\\/\\/\\S*', '', text, flags=re.MULTILINE)  # link\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+', ' ', text)  # menghilangkan @\n",
        "    text = re.sub(r'[^a-zA-Z0-9_]', ' ', str(text))  # huruf/angka/underline\n",
        "    text = re.sub(r'\\s\\s+', ' ', text)  # whitespace\n",
        "    text = re.sub(r'\\b\\w(1,2)\\b', ' ', text)  # batas antara kata dan non-kata\n",
        "    text = re.sub(r'[0-9]+', ' ', text)  # angka\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "  \n",
        "  return word_tokenize(text)\n",
        "\n",
        "def stopword_removal(text):\n",
        "  indonesian_stopwords = stopwords.words('indonesian')\n",
        "  indonesian_stopwords.extend(\n",
        "      [\n",
        "       \"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo',\n",
        "       'kalo', 'amp', 'biar', 'bikin', 'bilang',\n",
        "       'gak', 'ga', 'krn', 'nya', 'nih', 'sih',\n",
        "       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya',\n",
        "       'jd', 'jgn', 'sdh', 'aja', 'n', 't',\n",
        "       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
        "       '&amp', 'yah', 'jln', 'bs'\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  indonesian_stopwords = set(indonesian_stopwords)\n",
        "\n",
        "  text = [w for w in text if not w in indonesian_stopwords]\n",
        "  \n",
        "  return text\n",
        "\n",
        "def stemming(text):\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    text = [stemmer.stem(token) for token in text]\n",
        "    return text\n",
        "\n",
        "def preprocessing(dataset):\n",
        "  dataset['prepro'] = dataset['tweet'].apply(remove_punct)\n",
        "  dataset['prepro'] = dataset['prepro'].apply(tokenize)\n",
        "  dataset['prepro'] = dataset['prepro'].apply(stopword_removal)\n",
        "  \n",
        "  #Remove Duplicate\n",
        "  dataset = dataset[dataset.applymap(lambda x: x[0] if isinstance(x, list) else x).duplicated('prepro')]\n",
        "  \n",
        "  dataset['prepro'] = dataset['prepro'].swifter.apply(stemming)\n",
        "  \n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-i6Es81CNmL"
      },
      "source": [
        "total_tweet = 2000\n",
        "keyword = 'covid'\n",
        "\n",
        "# Crawling Data\n",
        "tweets = get_tweet(keyword, total_tweet)\n",
        "\n",
        "df = pd.DataFrame(data=tweets, columns=['id', 'user', 'location', 'tweet', 'created_at'])\n",
        "df\n",
        "\n",
        "# Dataset Preparation\n",
        "dataset = df[['id','user','tweet','created_at']]\n",
        "\n",
        "# Preprocessing\n",
        "cleaned_data = preprocessing(dataset)\n",
        "\n",
        "# Export Data\n",
        "cleaned_data.to_csv('Prepro1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}